### Lec1：Introduction

这门课程中，主要会讨论前两点：性能和容错

分布式系统的问题（挑战）在于:

1.因为系统中存在很多部分，这些部分又在并发执行，遇到并发编程和各种复杂交互所带来的问题，以及时间依赖的问题（比如同步，异步）。

2.分布式系统有多个组成部分，再加上计算机网络，会遇到一些意想不到的故障。由多台计算机组成的分布式系统，可能会有一部分组件在工作，而另一部分组件停止运行，或者这些计算机都在正常运行，但是网络中断了或者不稳定。

3.设计分布式系统的根本原因通常是为了获得更高的性能，比如说一千台计算机。但是实际上一千台机器到底有多少性能是一个棘手的问题，这里有很多难点。所以通常需要倍加小心地设计才能让系统实际达到期望的性能。

```
阅读MapReduce的论文
```

实验内容

```
lab1:MapReduce实验
lab2:实现Raft算法
lab3:使用我的Raft算法实现来建立一个可以容错的KV服务
lab4:把我的KV服务器分发到一系列的独立集群中，并通过运行这些独立的副本集群进行加速。同时，负责将不同的数据块在不同的服务器之间搬迁，并确保数据完整。即实现分片式KV服务，将数据在多个服务器上做分区，来实现并行的加速。
```

#### 基础架构

课程中主要介绍的一些基础架构。基础架构的类型主要是存储，通信（网络）和计算。

1.最关注的是存储，因为这是一个定义明确且有用的抽象概念，并且通常比较直观。人们知道如何构建和使用储存系统，知道如何去构建一种多副本，容错的，高性能分布式存储实现。

2.一些计算系统，比如MapReduce

3.一些关于通信的问题，但是主要的出发点是通信是我们建立分布式系统所用的工具。比如计算机可能需要通过网络相互通信，但是可能需要保证一定的可靠性，所以会提到一些通信。

对于存储和计算，目标是为了能够设计一些简单接口，让第三方应用能够使用这些分布式的存储和计算，这样才能简单的在这些基础架构之上，构建第三方应用程序。这里的意思是，我们希望通过这种抽象的接口，将分布式特性隐藏在整个系统内。这样从应用程序的角度来看就像一个文件系统或者一个大家知道如何编程的普通系统。我们希望构建一个接口，它看起来就像一个非分布式存储和计算系统一样，但是实际上又是一个有极高的性能和容错性的分布式系统。

#### 工具

当考虑这些抽象的时候，第一个出现的话题就是实现。人们在构建分布系统时，使用了很多的工具，例如：

1.RPC（Remote Procedure Call）。RPC的目标就是掩盖我们正在不可靠网络上通信的事实。

2.线程。使得我们可以利用多核心计算机。线程提供了一种结构化的并发操作方式，这样，从程序员角度来说可以简化并发操作。

3.因为我们会经常用到线程，我们需要在实现的层面上，花费一定的时间来考虑并发控制，比如锁

#### 性能

**可扩展性（Scalability）**

两台计算机构成的系统如果有两倍性能或者吞吐，就是我说的可扩展性。如果你构建了一个系统，并且只要增加计算机的数量，系统就能相应提高性能或者吞吐量。

相关论文或者系统的作者都在运行大型网站，而单个数据库或者存储服务器不能支撑这样规模的网站（所以才需要分布式存储）。

所以，有关扩展性是这样：我们希望可以通过增加机器的方式来实现扩展，但是现实中这很难实现，需要一些架构设计来将这个可扩展性无限推进下去

**容错**

大型分布式系统中有一个大问题，那就是一些很罕见的问题会被放大。例如在我们的1000台计算机的集群中，总是有故障，要么是机器故障，要么是运行出错，要么是运行缓慢，要么是执行错误的任务。一个更常见的问题是网络。在一个大规模分布式系统中，各个地方总是有一些小问题出现。

因为错误总会发生，必须要在设计时就考虑，系统能够屏蔽错误，或者说能够在出错时继续运行。同时，因为我们需要为第三方应用开发人员提供方便的抽象接口，它能够尽可能多的对应用开发人员屏蔽和掩盖错误。这样，应用开发人员就不需要处理各种各样的可能发生的错误。

**可用系统**通常是指，在特定的故障范围内，系统仍然能够提供服务，系统仍然是可用的。如果出现了更多的故障，系统将不再可用。

除了可用性之外，另一种容错特性是**自我可恢复性（recoverability）**

如果出现了问题，服务会停止工作，不再响应请求，之后有人来修复，并且在修复之后系统仍然可以正常运行，就像没有出现过问题一样。

为了实现这些特性，有很多工具。其中最重要的有两个：

1.非易失存储（non-volatile storage，类似于硬盘）。

2.对于容错的另一个重要工具是复制（replication），不过，管理复制的多副本系统会有些棘手。任何一个多副本系统中，都会有一个关键的问题，比如说，我们有两台服务器，它们本来应该是有着相同的系统状态。现在的关键问题在于，这两个副本总是会意外的偏离同步的状态，而不再互为副本。lab2和lab3都是通过管理多副本来实现容错的系统，这里很复杂。

**一致性（Consistency）**

一致性就是用来定义操作行为的概念。之所以一致性是分布式系统中一个有趣的话题，是因为，从性能和容错的角度来说，我们通常会有多个副本。在一个分布式系统中，由于复制或者缓存，数据可能存在于多个副本当中，于是就有了多个不同版本的key-value对。假设服务器有两个副本，那么他们都有一个key-value表单，两个表单中key 1对应的值都是20。

实际上，对于一致性有很多不同的定义。比如说get请求可以得到最近一次完成的put请求写入的值。这种一般也被称为**强一致（Strong Consistency）。**弱一致是指，不保证get请求可以得到最近一次完成的put请求写入的值。

虽然强一致可以确保get获取的是最新的数据，但是实现这一点的代价非常高。分布式系统的各个组件需要做大量的通信，才能实现强一致性。为了尽可能的避免通信，尤其当副本相隔的很远的时候，会构建**弱一致系统**，并允许读取出旧的数据。当然，为了让弱一致更有实际意义，还会定义更多的规则。

所以，为了保持强一致的通信，代价可能会非常高。因为每次你执行put或者get请求，你都需要等待几十毫秒来与数据的两个副本通信，以确保它们都被更新了或者都被检查了以获得最新的数据。现在的处理器每秒可以执行数十亿条指令，等待几十毫秒会大大影响系统的处理速度。

所以，人们常常会使用**弱一致系统**，你只需要**更新最近的数据副本**，并且只需要**从最近的副本获取数据**。在学术界和现实世界（工业界），有大量关于构建弱一致性保证的研究。所以，弱一致对于应用程序来说很有用，并且它可以用来获取高的性能。

### lec2: MapReduce

MapReduce的思想是，应用程序设计人员和分布式运算的使用者，只需要写简单的Map函数和Reduce函数，而不需要知道任何有关分布式的事情，MapReduce框架会处理剩下的事情。

抽象来看，MapReduce假设有一些输入，这些输入被分割成大量的不同的文件或者数据块。

MapReduce启动时，会查找Map函数。之后，MapReduce框架会为每个输入文件运行Map函数。这里很明显有一些可以并行运算的地方，比如说可以并行运行多个只关注输入和输出的Map函数。

Map函数以文件作为输入，文件又是整个输入数据的一部分。Map函数的输出是一个key-value对的列表。

**例子：**

```
假设我们在实现一个最简单的MapReduce Job：单词计数器。它会统计每个单词出现的次数。
Map函数会将输入中的每个单词拆分，并输出一个key-value对，key是该单词，value是1。最后需要对所有的key-value进行计数，以获得最终的输出。
所以，假设输入文件1包含了单词a和单词b，Map函数的输出将会是key=a，value=1和key=b，value=1。
第二个Map函数只从输入文件2看到了b，那么输出将会是key=b，value=1。
第三个输入文件有一个a和一个c。
我们对所有的输入文件都运行了Map函数，并得到了论文中称之为中间输出（intermediate output），也就是每个Map函数输出的key-value对。
```

运算的第二阶段是运行Reduce函数。MapReduce框架会收集所有Map函数输出的每一个单词的统计。

比如说，MapReduce框架会先收集每一个Map函数输出的key为a的key-value对。收集了之后，会将它们提交给Reduce函数。

之后会收集所有的b。这里的收集是真正意义上的收集，因为b是由不同计算机上的不同Map函数生成，所以不仅仅是数据从一台计算机移动到另一台（如果Map只在一台计算机的一个实例里，可以直接通过一个RPC将数据从Map移到Reduce）。我们收集所有的b，并将它们提交给另一个Reduce函数。这个Reduce函数的入参是所有的key为b的key-value对。对c也是一样。所以，MapReduce框架会为所有Map函数输出的每一个key，调用一次Reduce函数。

最后，每个Reduce都输出与其关联的单词和这个单词的数量。所以第一个Reduce输出a=2，第二个Reduce输出b=2，第三个Reduce输出c=1。

这就是一个典型的MapReduce Job。

- Job：整个MapReduce计算称为Job。

- Task：每一次MapReduce调用称为Task。

对于一个完整的MapReduce Job，它由一些Map Task和一些Reduce Task组成。

#### MAP函数

Map函数使用一个key和一个value作为参数。入参中，key是输入文件的名字，value是输入文件的内容。

1.对于一个单词计数器来说，value包含了要统计的文本，我们会将这个文本拆分成单词

2.对于每一个单词，我们都会调用emit。emit会接收两个参数，其中一个是key，另一个是value。emit入参的key是单词，value是字符串“1”。

```
map(k,v)
	split v into word
	for each word w
		emit(w,'1')
```

在一个单词计数器的MapReduce Job中，Map函数实际就可以这么简单。而这个Map函数不需要知道任何分布式相关的信息，不需要知道有多台计算机，不需要知道实际会通过网络来移动数据。这里非常直观。

#### Reduce函数

Reduce函数的入参是某个特定key的所有实例（Map输出中的key-value对中，出现了一次特定的key就可以算作一个实例）。

Reduce函数也是使用一个key和一个value作为参数，其中value是一个数组，里面每一个元素是Map函数输出的key的一个实例的value。对于单词计数器来说，key就是单词，value就是由字符串“1”组成的数组。

Reduce函数也有一个属于自己的emit函数。这里的emit函数只会接受一个参数value，这个value会作为Reduce函数入参的key的最终输出。对于单词计数器，我们会给emit传入数组的长度。

```
Reduce(k,v)
	emit(len(v))
```

这就是一个最简单的Reduce函数。并且Reduce也不需要知道任何有关容错或者其他有关分布式相关的信息。

#### Others

1.将MapReduce的输出作为另一个MapReduce Job的输入这很正常。

2.Map函数必须是完全独立的，它们是一些只关心入参的函数。

3.现实中，MapReduce运行在大量的服务器之上，我们称之为worker服务器或者worker。同时，也会**有一个Master节点来组织整个计算过程**。这里实际发生的是，Master服务器知道有多少输入文件，例如5000个输入文件，之后它将Map函数**分发到不同的worker**。所以，它会向worker服务器发送一条消息说，请对这个输入文件执行Map函数吧。之后，MapReduce框架中的worker进程会读取文件的内容，调用Map函数并将文件名和文件内容作为参数传给Map函数。worker进程还需要实现emit，这样，每次Map函数调用emit，worker进程就会**将数据写入到本地磁盘的文件**中。所以，Map函数中调用emit的效果是在worker的本地磁盘上创建文件，这些文件包含了当前worker的Map函数生成的所有的key和value。

4.Map阶段结束时，我们看到的就是Map函数在worker上**生成的一些文件**。之后，MapReduce的worker会将这些数据**移动到Reduce所需要的位置**。对于一个典型的大型运算，Reduce的入参包含了所有Map函数对于特定key的输出。通常来说，每个Map函数都可能生成大量key。所以通常来说，在运行Reduce函数之前。运行在MapReduce的worker服务器上的进程需要与集群中每一个其他服务器交互来询问说，我需要对key=a运行Reduce，请看一下你本地磁盘中存储的Map函数的中间输出，找出所有key=a，并通过网络将它们发给我。所以，Reduce worker需要**从每一个worker获取特定key的实例**。这是通过由**Master通知**到Reduce worker的一条指令来触发。一旦worker收集完所有的数据，它会调用Reduce函数，Reduce函数运算完了会调用自己的emit，这个emit与Map函数中的emit不一样，它会将输出**写入到一个Google共享文件服务**中。

5.输入和输出文件都存放在文件中，因为我们想要**在任意的worker上读取任意的数据**，这意味着我们需要某种**网络文件系统**（network file system）来存放输入数据。GFS是一个共享文件服务，并且它也运行在MapReduce的worker集群的物理服务器上。GFS会自动拆分你存储的任何大文件，并且以64MB的块存储在多个服务器之上。所以，如果你有了10TB的网页数据，你只需要将它们写入到GFS，甚至你写入的时候是作为一个大文件写入的，**GFS会自动将这个大文件拆分成64MB的块**，并将这些块**平均的分布在所有的GFS服务器**之上，这正是我们所需要的。如果我们接下来想要对刚刚那10TB的网页数据运行MapReduce Job，数据已经均匀的分割存储在所有的服务器上了。如果我们有1000台服务器，我们会**启动1000个Map worker**，每个Map worker会读 取1/1000输入数据。这些**Map worker可以并行的从1000个GFS文件服务器读取数据**，并获取巨大的读取吞吐量，也就是1000台服务器能提供的吞吐量。

6.如果**随机的选择MapReduce的worker服务器和GFS服务器**，那么至少有一半的机会，它们之间的通信需要经过root交换机，而这个root交换机的吞吐量总是固定的。如果做一个除法，root交换机的总吞吐除以2000，那么每台机器只能分到50Mb/S的网络容量。这个网络容量相比磁盘或者CPU的速度来说，要小得多。所以，50Mb/S是一个巨大的限制。

7.在MapReduce论文中，讨论了大量的**避免使用网络**的技巧。其中一个是**将GFS和MapReduce混合运行在一组服务器**上。所以如果有1000台服务器，那么GFS和MapReduce都运行在那1000台服务器之上。当MapReduce的Master节点拆分Map任务并分包到不同的worker服务器上时，**Master节点会找出输入文件具体存在哪台GFS服务器上，并把对应于那个输入文件的Map Task调度到同一台服务器上**。所以，默认情况下，这里的箭头是指读取本地文件，而不会涉及网络。虽然由于故障，负载或者其他原因，不能总是让Map函数都读取本地文件，但是几乎所有的Map函数都会运行在存储了数据的相同机器上，并因此节省了大量的时间，否则通过网络来读取输入数据将会耗费大量的时间。

8.有时确实需要将每一份数据都通过网络从创建它的Map节点传输到需要它的Reduce节点。论文里称这种数据转换之为洗牌（shuffle）。这也是MapReduce中代价较大的一部分。

